### MLP

![[Pasted image 20260216003538.png|100x100]]
**MultiLayer Perceptron**(MLP)의 예시 이미지
![[Pasted image 20260214002527.png|600x450]]

여기 그림에 보이는 각 원을 **뉴런**이라고 하고, **activation**값을 갖고 있다
이 그림에서 각 뉴런의 Activation값은 픽셀이 얼마나 밝은지를 나타내고 있다
0-9까지의 숫자가 적혀있는 마지막 layer의 각 뉴런은 현재 input이 해당 숫자일 **확률을** 나타낸다
가운데 두 layer는 **Hidden Layer**(은닉층) 이라고 한다

이전 레이어의 activation값이 네트워크를 통과해 다음 layer의 값이 정해지며 최종 결과물까지 닿게 된다. 이는 인간의 시각, 청각 피질의 구조를 모방한 것이다.

---
### **Layer**?
사람이 숫자를 그림과 같이 인식한다고 생각해보자
![[Pasted image 20260216004508.png|300x200]]

그러면 출력층 이전 레이어에서 다음과 같은 패턴을 들고 있기를 기대한다고 볼 수 있다
![[Pasted image 20260216004543.png|400x450]]

그리고 그보다 더 이전 레이어에서는,
![[Pasted image 20260216004823.png|300x100]]

이렇게 뚜렷한 원, 긴 직선 등의 패턴 이전의 더 작은 조각을 나타내는 값을 갖는다 할 수 있다.
![[Pasted image 20260216004847.png|300x300]]

이렇게 신경망 모델을 통해 패턴을 인식하기를 희망한다.
비슷한 예시로 오디오 해석을 생각해 볼 수 있다
![[Pasted image 20260216005127.png]]

---

### **Weight, Bias, Parameter**

![[Pasted image 20260216004847.png|300x300]]

이 그림에서 layer 사이에 뉴런끼리 연결되어 있는 선이 **가중치이다**
activation값들이 어떻게 조합될지를 정하는 역할을 한다

![[Pasted image 20260216005633.png|400x250]]

이전 layer의 모든 actiavtion과 weight를 곱해서 더하면 다음 layer 하나의 activation값이 된다.
이 때, activation 값을 0~1 사이로 하기 위해 **sigmoid** function을 사용한다
![[Pasted image 20260216011646.png|400x300]]
작은 값일수록 0, 클 수록 1에 가까워지는 함수이다.![[Pasted image 20260216011811.png]]

이렇게 weighted sum을 sigmoid에 대입하는데, 이 때 **유의미한 임계값**이 있다면 그 값을 나타내기 위해 weighted sum을 빼주는데, 이를 **bias**라고 한다.

이제 이 모든 784개의 뉴런에 대해 이 연산을 수행해 다음 layer의 activation을 구한다.

![[Pasted image 20260216012125.png|500x400]]
![[Pasted image 20260216012150.png|600]]

이 모든 weight, bias를 묶어 **parameter**라고 하고, 예시에서는 13002개가 있다

**훈련의 목표** : 적절한 weight와 bias를 찾는 것

---

### Notation
![[Pasted image 20260216012631.png]]
가중치를 행렬로, activation을 벡터로 두면 이 둘의 곱셈과 동일하다.

![[Pasted image 20260216012823.png]]
![[Pasted image 20260216012840.png]]
이렇게 나온 결과가 다음 layer의 activation이 된다