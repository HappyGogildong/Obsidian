
### Cost(Loss)
![[Pasted image 20260219170826.png]]
처음 신경망을 만들면, weight, bias가 랜덤하게 초기화되어있는데, 이 신경망에 통과시켜서 나온 쓰레기 값과 실제 정답과의 차이를 **cost**라고 한다.

![[Pasted image 20260219171033.png]]
cost는 이렇게 실제 정답과의 차이를 각각 제곱해서 더한 값인데, 다른 말로는 **loss**라고 한다.
신경망의 목표는 모든 훈련 데이터에 대한 평균 loss가 낮아야 한다.


![[Pasted image 20260219174359.png]]
그리고, cost(loss)를 이렇게 weight, bias를 인자로 하는 함수(**손실 함수**)로 생각해볼 수 있고,
loss를 낮춘다는 것은 이 **함수의 최솟값을 찾는 문제**라 생각해볼 수 있겠다.

### Gradient Descent
![[Pasted image 20260219174607.png]]
w가 입력되었을 때의 기울기를 보고 +라면 왼쪽으로, -라면 오른쪽으로 입력값을 이동시켰을 때
국소 최솟값을 찾을 수 있다. 조금 더 차원을 늘려봤을 때도 비슷한 방식을 적용할 수 있다.
어느 방향으로 갔을 때 기울기가 가장 가파르게 변하는 지.
![[Pasted image 20260219175111.png]]
![[Pasted image 20260219175153.png]]
그래서 이 과정을 통해 수 많은 파라미터를 가진 손실 함수의 국소값을 찾을 수 있다.
![[Pasted image 20260219175455.png]]
$\vec w$는 함수의 가중치들을 벡터로 표현한 것임.
그리고 손실함수를 각 가중치에 대해 편미분하면, 해당 가중치를 어느 방향으로 얼마나 조정해야 손실이 가장 빠르게 감소하는지를 알 수 있게 된다.

