앞에서 본 것처럼, 수학 올림피아드 문제처럼 고난이도의 문제를 해결할 수 있음에도
간단한 문자열 세기나 9.11과 9.9중 무엇이 큰가? 와 같은 문제를 틀리는 들쭉날쭉한(Jagged)
면을 보인다.

이렇게 LLM은 완전히 신뢰할 수 없는 확률적 시스템이기에 결과를 복사 붙여넣기하는 것이 아닌
검증이 필요한 도구로써 바라보아야 한다.


> **Jagged intelligence는  
> “연속적인 세계를  
> 이산 토큰·국소 통계·고정 계산량으로  
> 근사하려는 시스템의 필연적 산물”이다.**
## GPT : jagged intelligence의 원인 분석

**Jagged intelligence**(지그재그 지능, 울퉁불퉁한 지능)의 핵심은

> _“모델의 내부 표현·최적화·평가 기준이 인간이 기대하는 ‘연속적 능력 축’과 정렬되어 있지 않다”_  
> 는 데 있다.

아래를 **원인 계층별로** 정리하겠다. (표면적 현상 → 구조적 원인 → 근본 원인)

---

### 1. 현상 정의 (증상)

Jagged intelligence란 다음과 같은 상태다.

- 어려운 문제는 잘 푸는데 쉬운 문제에서 실수
- 고급 추론은 되는데 상식 질문에서 오류
- 한 포맷에서는 완벽, 약간만 바꾸면 붕괴
- 같은 개념을 다른 표현으로 물으면 성능 급락

즉,

> **능력이 “연속 스칼라”가 아니라 “불연속 패턴”처럼 보이는 현상**

---

### 2. 1차 원인: 학습 목표의 비정합 (Objective Misalignment)

#### 2.1 언어모델의 실제 목표

LM이 최적화하는 것은:

![[Pasted image 20260204224057.png]]
이 목표는 다음을 보장하지 않는다:

- 개념의 일관성
    
- 난이도 단조성
    
- 문제 구조의 보존

👉 **“쉬운 문제 ⊂ 어려운 문제”라는 관계가 loss에 존재하지 않는다**

---

### 2.2 난이도는 모델이 모르는 인간적 개념

인간 기준:

- 초등 문제 < 대학 문제
    

모델 기준:

- **토큰 분포가 흔한가?**
    
- **학습 데이터에서 패턴이 명확한가?**

그래서:

- “어려워 보이지만 자주 등장한 패턴” → 잘함
    
- “쉬워 보이지만 희귀한 표현” → 못함
---

### 3. 2차 원인: 표현 공간의 국소적 일반화 (Local Generalization)

#### 3.1 개념은 점이 아니라 “섬”으로 존재한다

신경망 내부에서 개념은:

- 연속적인 수학적 객체 ❌
    
- **고차원 공간의 조밀하지 않은 클러스터** ⭕

결과:

- A 표현 → B 표현으로의 일반화가 보장되지 않음
    
- 의미적으로 동일하지만 통계적으로 먼 경우 실패

---

#### 3.2 “개념 이해”의 환상

모델은 보통:

- 개념 C를 **규칙으로 저장**하지 않고
    
- C와 관련된 **표현들의 공분산 구조**를 저장

그래서:

- 훈련 분포 내부 → 매우 강함
    
- 경계 근처 → 급격한 붕괴

👉 이게 jaggedness의 핵심 수학적 형태다.

---

### 4. 3차 원인: 토큰 단위 계산 제한 (Compute per Token Bottleneck)

네가 이전에 말한 이 문장이 정확히 여기를 찌른다:

> “모든 개별 토큰이 유한한 양의 계산만 사용한다”

#### 4.1 단일 토큰의 계산 한계

Transformer에서 한 토큰이 할 수 있는 일은:

- 고정된 depth × width의 연산
    
- **추가 계산을 요청할 수 없음**
    

즉:

- “조금 더 생각하기”가 구조적으로 불가능
    

---

#### 4.2 결과: 조기 수렴 vs 계산 부족

- 어떤 문제는 **초기 패턴 매칭만으로 정답**
    
- 어떤 문제는 **중간 추론 토큰이 필요**
    

하지만:

- 모델이 이를 사전에 정확히 구분 못함
    
- → 어떤 문제는 충분히 계산
    
- → 어떤 문제는 계산 없이 결론
    

이로 인해:

> **난이도와 실제 계산량이 비동기화**

---

### 5. 4차 원인: RLHF / Preference Learning의 부작용

#### 5.1 보상은 “정확성”이 아니라 “선호”다

RLHF는 보통 다음을 강화한다:

- 그럴듯함
    
- 자신감 있는 어조
    
- 빠른 결론
    
- 간결한 설명
    

👉 **“모르겠다”는 표현은 구조적으로 불리**

---

#### 5.2 Jaggedness를 더 심화시키는 이유

- 쉬운 질문:
    
    - 모델이 _확신을 가져야 한다_고 학습됨
        
    - 틀려도 낮은 entropy로 출력
        
- 어려운 질문:
    
    - 조심스러운 패턴이 허용됨
        

그래서:

> **쉬운 질문에서 더 단호하게 틀림**

---

### 6. 5차 원인: 평가 함수의 불연속성

#### 6.1 실제 성능은 연속인데, 관측은 불연속

모델 내부:

- logit 차이는 연속적
    

우리가 보는 것:

- 정답 / 오답 (step function)
    

그래서:

- 작은 logit 변화 → 정답 ↔ 오답 급변
    
- → 지능이 “튀는 것처럼” 보임
    

---

### 7. 근본 원인 요약 (한 문장)

> **Jagged intelligence는  
> “연속적인 세계를  
> 이산 토큰·국소 통계·고정 계산량으로  
> 근사하려는 시스템의 필연적 산물”이다.**

---

#### 8. 왜 이 현상은 쉽게 사라지지 않는가

- 더 큰 모델 → 완화는 됨
    
- 하지만:
    
    - objective는 동일
        
    - 토큰 계산 구조 동일
        
    - 분포 외 일반화 문제 유지
        

그래서:

> **스케일링은 jaggedness를 줄이지, 제거하지는 않는다**

---

### 9. Jagged intelligence를 줄이려는 실제 시도들

|접근|목적|
|---|---|
|CoT / Deliberate tokens|계산 분산|
|Adaptive compute|토큰별 계산량 조절|
|DPO / RLAIF|과도한 확신 완화|
|Self-consistency|국소 오류 평균화|
|Tool use|내부 계산 한계 외부화|

하지만:

- **완전 해결은 아직 없음**
    

---

### 10. 마지막 정리 (네 문제의식과 연결)

네가 던진 일련의 질문들은 사실 하나의 축으로 연결된다:

- 토큰당 계산 제한
    
- entropy 낮은 오답
    
- RLHF의 구조
    
- CoT의 필요성
    
- jagged intelligence
    

이들은 전부:

> **“언어모델은 사고하는 존재가 아니라  
> 계산을 토큰에 분산시키는 확률 시스템”**

이라는 사실에서 파생된다.

원하면 다음 단계로:

- jagged intelligence와 **OOD generalization**의 수학적 연결
    
- “entropy가 낮은 오답”의 logit 관점 분석
    
- 왜 **adaptive depth Transformer**가 어려운지
