LLM에 
`각 오렌지는 2달러이고, 모든 과일의 총 가격은 13달러입니다. 사과 하나의 가격은 얼마일까요?`
와 같은 질문을 주고, 라벨링을 한다고 하면 다양한 풀이를 생각해볼 수 있다.

하지만 문제에 답을 구할 때, 더 펼쳐서 쓰는 게 나을 수도 있고, 방정식으로 세우는 게 나을 수도 있고, 설명하며 푸는 게 나을 수도 있다.
이 중 어느 방식이 더 나은 것인지 사람은 잘 알 수 없는데, 왜냐하면 우리에게 쉽거나 어려운 것과 LLM에게 쉽거나 어려운 것이 다르기 떄문이다. **인지 방식이 다르다**.

사람이 만드는 많은 토큰이 LLM에 낭비일 수 있거나 사람이 LLM의 매개변수에 없는 지식을 넣으면 그것은 모델에게 혼란스러운 도약이 될 수 있다.

결론적으로, 사람은 LLM을 위한 최적의 토큰 시퀀스를 만드는 데 적합하지 않고, 
프롬프트가 주어졌을 때 신뢰가능한 정답에 도달하는 토큰 시퀀스를 LLM 스스로 찾아나가야 하며, 이를 ==**강화학습**==과 **시행착오 과정**에서 발견해야 한다.

![[Pasted image 20260205212903.png]]
프롬프트에 문제가 주어지고, 15개의 풀이 중 4개의 정답을 맞추었다고 하자.
이제 이 녹색 4개의 풀이를 장려하고자 한다. 그러니, 이 시퀀스들을 더 훈련시킨다.
이때 이 훈련 시퀀스는 기존과 달리 사람이 라벨링한 것이 아닌, **모델 자체에서** 온 것이다.
모델이 15가지 풀이를 시도 했고, 4개가 유효했고, 그것들을 훈련하기 시작한다.

이는 사람에 비유하면 "괜찮은 풀이를 찾았으니, 이런 문제는 이렇게 풀어야겠다"고 생각하는 것과 비슷하다. 여기서도 방법론을 조정하는 많은 방법들이 있지만, 네 개 중 가장 좋은 풀이 하나를 선택한다고 가정하자. 그 외에도 풀이의 길이나 가독성 등 다양한 metric을 잡을 수 있다. 

이 작업은 다양한 프롬프트에서 실행된다는 것을 기억해야 한다. 광범위한 주제에 대해 수만 개의 프롬프트가 있고 프롬프트당 수천 개의 풀이가 있다. 이 과정을 반복하며 모델은 어떤 토큰 시퀀스가 정답에 도달하는지 스스로 발견한다.

모델이 이 놀이터에서 놀면서 목표를 알고 자신에게 맞는 시퀀스를 발견한다. 정신적 도약을 하지 않고 신뢰할 수 있게 통계적으로 작동하며 모델의 지식을 완전히 활용하는 시퀀스이다. 이것이 **Reinforcement Learning**이다.
기본적으로 추측하고 확인하는 것입니다. 다양한 유형의 풀이를 추측하고, 확인하고, 효과가 있었던 것을 미래에 더 합니다.

이전에 사용한 SFT는 모델을 올바른 풀이의 근처로 초기화하는데 도움이 된다. 
