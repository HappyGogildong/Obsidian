인간이 톰 크루즈가 누구냐, 존 바라소가 누구냐 등의 질문에 대한 정답을 작성할 때
인간은 이 사람이 누군지 알거나, 인터넷에서 서칭을 통해 확신 있는 톤의 응답을 작성하게 된다.

그러면 실존하지 않는 사람에 대해 물어보게 되면 내부 activation값이나 어떤 부분에서 그 사람을 모르는 것을 알고 있다고 하더라도 "모르겠습니다"라는 답변 대신 최대한 통계적으로 가장 가능성 높은 추측을 준다(지어냄).

Falcon 7B같이 오래된모델에 "오손 코바츠가 누구냐"라고 하면, 모델은 "오손 코바츠가 누구냐 어시스턴트"라는 토큰으로 시작해 추론을 시작하고, 답변 스타일과 통계적으로 일치하는 답변을 내뱉는다. 모델은 기본적으로 답변 형식을 모방하고 있으며, 인터넷에 찾으러 가지 않는다.

하지만 최신 ChatGpt에 동일한 질문을 하면 잠깐 웹 검색 중이라 하며, 오손 코바츠라는 알려진 인물이 없다는 것을 알린다. 

그러면 이 현상을 어떻게 고칠까? 데이터셋에 어시스턴트의 정답이 "모른다"인 예시가 필요한데,
실제로 모르는 경우에만 나오도록 해야 한다. 다시 말해 모델이 무엇을 알고 모르는 지 알아내는 것이 필요하다.
경험적으로 모델을 조사해 알아낼 수 있다.
기본적으로 모델을 심문해서 알고 모르는 것에 대한 경계를 파악한 뒤, 모르는 것에 대해 정답이 모델이 모른다는 것인 예시를 학습 세트에 추가한다.

이 간단한 방법이 왜 문제를 해결할 수 있을까?
그 이유는 네트워크 내부에 자기 지식에 대한 괜찮은 모델을 갖고 있기 때문이다.
네트워크와 내부의 뉴런을 보았을 때, 모델이 불확실할 때 켜지는 뉴런이 존재할 것이라 상상할 수 있다. 하지만, 해당 뉴런의 활성화와 모델이 모른다고 말로 하는 것이 연결이 되어 있지 않다.
신경망의 내부가 모른다는 것을 알고 있더라도, 모델은 그것을 드러내지 않고 학습 세트에서 봤던 확신 있는 대답을 위해 추측을 한다. 
그래서 불확실한 뉴런이 켜지는 문제와 그 대답이 "모른다"인 데이터를 학습시키는 것이다.
Meta가 Llama3에서 환각을 다루는 방법을 살펴보자.

학습 세트의 랜덤 문서를 가져와 문단을 구성한 다음, LLM을 사용해 문단에 대한 질문을 구성한다. 그렇게 질문과 답을 가져오고, 모델에 여러 번 물어보며 알고 있는지 심문한다.
그렇게 하다가 모르는 질문을 발견하면 이 질문으로 새 대화를 학습 세트에 넣는다. 이 대화에서 어시스턴트의 대답은 "모르겠습니다"이다.

그 다음 방법은 모른다는 대답 대신 인터넷을 검색하고 답을 찾아 대답하는 것이다.
![[Pasted image 20260204025652.png]]
이 것을 위해 새로운 토큰 SEARCH_START, SEARCH_END를 추가로 도입해 
그러면 추론을 실행하다 이 토큰을 보면 다음 토큰 샘플링 대신 나가서 검색을 하고 검색된 텍스트를 가져온다. 그 뒤 다시 토큰으로 표현하고 컨텍스트 윈도우에 들어간다.
이 컨텍스트 윈도우는 모델의 작업 메모리라고 생각하면 된다.
그러면 이 컨텍스트 윈도우에서 정답을 찾아 말할 수 있다.

신경망의 파라미터에 있는 지식은 희미한 기억, 컨텍스트 윈도우를 구성하는 토큰의 지식은 작업 메모리이다. 그래서 LLM과 상호작용할 때