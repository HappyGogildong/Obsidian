인간이 톰 크루즈가 누구냐, 존 바라소가 누구냐 등의 질문에 대한 정답을 작성할 때
인간은 이 사람이 누군지 알거나, 인터넷에서 서칭을 통해 확신 있는 톤의 응답을 작성하게 된다.

그러면 실존하지 않는 사람에 대해 물어보게 되면 내부 activation값이나 어떤 부분에서 그 사람을 모르는 것을 알고 있다고 하더라도 "모르겠습니다"라는 답변 대신 최대한 통계적으로 가장 가능성 높은 추측을 준다(지어냄).

Falcon 7B같이 오래된모델에 "오손 코바츠가 누구냐"라고 하면, 모델은 "오손 코바츠가 누구냐 어시스턴트"라는 토큰으로 시작해 추론을 시작하고, 답변 스타일과 통계적으로 일치하는 답변을 내뱉는다. 모델은 기본적으로 답변 형식을 모방하고 있으며, 인터넷에 찾으러 가지 않는다.

하지만 최신 ChatGpt에 동일한 질문을 하면 잠깐 웹 검색 중이라 하며, 오손 코바츠라는 알려진 인물이 없다는 것을 알린다. 

그러면 이 현상을 어떻게 고칠까? 데이터셋에 어시스턴트의 정답이 "모른다"인 예시가 필요한데,
실제로 모르는 경우에만 나오도록 해야 한다. 다시 말해 모델이 무엇을 알고 모르는 지 알아내는 것이 필요하다.
경험적으로 모델을 조사해 알아낼 수 있다.
Meta가 Llama3에서 환각을 다루는 방법을 살펴보자.



