카파시는 GPT-2가 현대적인 트랜스포머 기술 스택이 처음으로 모인 모델이라 예시로서 사용했음.

2019년에 만들어진 GPT-2는 16억개의 파라미터와 1024 토큰의 컨텍스트를 사용. 
2025.2.6 기준 최신 트랜스포머는 1조- 수천 억 파라미터와 수십에서 수백만 컨텍스트를 사용.

https://github.com/karpathy/llm.c
카파시의 600$로 GPT-2를 재현하는 프로젝트

**비용 절감의 이유?**
1. 데이터셋이 훨씬 좋아짐
	[[2. Pretraining Data(Internet)]] 에서 살펴본 필터링, 추출, 준비 과정이 발전했다
2. 하드웨어가 훨씬 빨라짐
3. 하드웨어를 활용하는 소프트웨어를 발전시킴

![[Pasted image 20260202011001.png]]
이 그림에서 한 줄 한 줄이 업데이트.(가중치와 파라미터 업데이트)
한 줄에 훈련 세트 100만 개 토큰에 대한 예측을 개선한다.

여기서 loss는 신경망이 얼마나 잘 수행하고 있는지를 알려주는 단일 숫자이다.
낮을수록 좋다