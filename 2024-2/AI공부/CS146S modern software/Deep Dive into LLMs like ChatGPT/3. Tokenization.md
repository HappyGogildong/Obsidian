
신경망은 1차원 기호 시퀀스를 입력으로써 기대한다.

첫 번째로, 인터넷에서 얻은 텍스트를 UTF-8 인코딩한다.
그러고나면 0과1로 이루어진 매우 긴 시퀀스를 얻게 되는데, 
0과 1외의 다른 기호를 추가해 기호 크기와 시퀀스 길이 사이의 트레이드오프를 맞춘다.

간단한 방법은 연속된 8비트를 하나의 바이트로 묶는다. 
그러면 8길이의 시퀀스를 최대 3의 길이를 가진 시퀀스로 압축이 가능하다. 
2**개의 기호와 매우 긴 시퀀스 -> 256개의 기호와 짧아진 시퀀스**

**바이트 페어 인코딩 알고리즘**은 매우 흔한 연속된 바이트나 기호를  찾는 것이다.
예를 들어 116 32 이렇게 오는 시퀀스가 매우 흔하고 자주 발생되면, 이 시퀀스를
하나의 기호로 만들고 새로 만든 기호로 대체한다. 

실제로 약 10만 개 정도의 기호가 좋은 성능을 보인다고 한다.
GPT-4는 100,277개의 기호를 사용.

이렇게 raw text를 token(기호)로 변환하는 과정을 **Tokenization**이라고 한다
tiktokenizer에서 GPT-4가 토큰화하는 과정을 볼 수 있다.


