
LLM을 만들기 위한 첫 번째 단계는 **사전학습**인데,
여기서는 사전학습의 첫 단계, **인터넷 데이터 수집과 처리** 과정에 대해 설명한다

https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1
Huggingface에서 FineWeb이라는 데이터셋 만든 과정 링크

Common Crawl은 2007년부터 27억개의 웹페이지를 수집해왔는데, 
인터넷을 돌아다니는 크롤러들이 몇 개의 시드 웹페이지에서 시작해서 링크를 따라가고 정보를 색인해 인터넷의 정보를 얻게된다.

![[Pasted image 20260129233922.png]]
이렇게 얻은 Common Crawl의 데이터는 raw해서 필터링을 거쳐야한다.
첫 번째는 **URL Filtering**. 데이터를 얻고 싶지 않은 URL. 스팸, 도박, 성인 사이트 같은 것들을 걸러내는 작업이다

두 번째는 **Text Extraction**. 크롤러가 추출해낸 raw HTML은 각종 헤더와 마크업들이 있는데, 이는 웹페이지 동작을 위한 컴퓨터 코드이다. 그 중 텍스트 내용만 추출해내는 과정이다.

세 번쨰는 **Language Filtering**. FineWeb은 영어가 65%이상인 웹페이지만 유지하는데, 이 설정은 회사마다 다르게 결정할 수 있다. 어떤 언어를 모두 필터링하게 되면 모델이 그 언어를 못하게 된다. 그 후에 여러 필터링, 중복 제거 과정을 거치게 된다.

**PII Removal**은 개인식별정보를 필터링하는 과정이다. 주소, 사회보장번호 등과 같은 정보.

이렇게 필터링 된 텍스트 약 40TB를 갖게 된다.
이 텍스트 데이터에는 패턴들이 있고, 이 데이터로 신경망을 훈련시켜서 신경망이 
텍스트의 패턴을 내재화하고 모델링하게 하는 것이 목표이다.

그 다음에는 이 텍스트를 신경망에 넣기 전, 텍스트를 어떻게 표현하고 입력하리 