지금까지의 과정은 구체적인 답이 있고, 풀이를 만들어 확인하는 루프였다.
하지만 **검증 불가능한 영역**에서는 이 전략을 사용할 수 없다.

예를 들어, 창작 글쓰기 작업이 있다.
펠리컨에 대한 농담을 쓰거나, 시를 쓰거나, 문단을 요약하기.

`펠리컨에 대한 농담을 써줘` 라는 프롬프트에 ChatGPT는 좋은 유머를 생각해내지 못하는데,
유머는 꽤 어렵다.

문제는 모델이 만들어낸 유머들에 어떻게 점수를 매기느냐는 것이다.
사람이 모든 농담을 보고 점수를 매길 수 있지만 수 천개의 응답을 생성하는 수 천개의 프롬프트를 수 천번에 업데이트에 모두 사람이 관여할 수는 없다.

**RLHF** 접근법을 사용한다.
사람을 약간 참여시켜서 **보상 모델**이라고 부르는 별개의 신경망을 훈련시키는 것이다.
사람에게 롤아웃을 점수 매기도록 요청하고 신경망을 사용해서 사람 점수를 모방한다.
그러고나면 실제 사람에게 묻는 대신 시뮬레이션된 사람(보상 모델)에 묻는다.
완벽한 인간은 아니지만, 통계적으로 인간 판단과 유사할 것이다.

보상 모델 훈련을 다음 그림을 통해 간단히 살펴보자.
![[Pasted image 20260206001433.png]]
프롬프트에 다섯 개의 롤아웃을 받고, 사람이 이 농담의 순위를 매긴다.
그리고 별개로, 보상 모델에 프롬프트와 농담을 전달하고 이 농담들에 점수를 매기도록 한다.
그리고 나서 보상 모델이 준 점수와 사람이 준 순서를 비교한다.

이 계산은 손실 함수를 설정하고 일치도를 계산하고 그것에 기반해 모델을 업데이트한다.
예시에서 사람이 2등을 준 농담의 점수가 너무 낮다. 그러면 해당 농담은 업데이트 이후 점수가 0.15 정도로 올라갈 수 있다. 신경망 훈련 과정을 사용해 모델의 예측을 조정하고, 인간 순서와 일치하게 한다. 그렇게 업데이트를 거치면 인간 선호도의 시뮬레이터로서 사용할 수 있게 된다.

RLHF의 장점으로는
RLHF를 사용하면 검증 불가능한 것들을 포함한 영역에서 강화학습을 할 수 있게 해준다.
안드레이의 경험적으로 RLHF를 사용하면 모델 성능이 향상되었는데, 판별자-생성자 격차 때문이라고 추측하고 있다.

SFT 방식을 사용하고자 할 경우, 라벨러에 직접 시를 쓰라는 요청을 해야 하는데, 
RLHF에서처럼 모델에게 다섯 개의 시를 받고 순서를 매기는게 훨씬 더 쉬운 작업이다.

이것이 기본적으로 더 높은 정확도의 데이터를 허용하는데, 매우 어려운 생성 작업 대신
창의적 글쓰기를 구별하고 더 좋은 것을 찾으라고 한다. 이는 사람이 제공하는 신호, 순서고
시스템에 대한 입력이다. 그 다음 RLHF 시스템이 사람ㅁ에게 좋은 평가를 받을 수 있는 응답을 발견한다. 

하지만 단점도 있다.
주요한 것은 실제 인간의 판단이 아닌 인간의 손실 있는 시뮬레이션에 대해 강화학습을 한다는 것이다.  점수를 출력하는 언어 모델이고, 실제 사람의 의견을 완벽하게 반영하지 못한다.

그리고 강화학습은 시뮬레이션, 모델을 game하는 방법을 발견하는데 뛰어나다

보상 모델은 수십 억 매개변수로 이루어진 트랜스포머인데, 이 모델이 단일 점수를 출력한다.
그리고 이 모델을 게임하는 방법이 있다는 것이 밝혀졌다. 의도된 답이 아닌 꼼수 같은 거임.
업데이트를 할 수록 농담이 더 좋아지리라 기대할 수 있지만, 실제로는 몇 백 단계에서 개선되다가 극적으로 떨어지게 된다.

예를 들어 펠리컨에 대한 농담이 "the the the"가 되기 시작한다.
이 예시를 적대적 입력이라고 한다.
