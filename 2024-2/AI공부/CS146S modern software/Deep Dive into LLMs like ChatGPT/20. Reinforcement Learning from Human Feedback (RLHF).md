지금까지의 과정은 구체적인 답이 있고, 풀이를 만들어 확인하는 루프였다.
하지만 **검증 불가능한 영역**에서는 이 전략을 사용할 수 없다.

예를 들어, 창작 글쓰기 작업이 있다.
펠리컨에 대한 농담을 쓰거나, 시를 쓰거나, 문단을 요약하기.

`펠리컨에 대한 농담을 써줘` 라는 프롬프트에 ChatGPT는 좋은 유머를 생각해내지 못하는데,
유머는 꽤 어렵다.

문제는 모델이 만들어낸 유머들에 어떻게 점수를 매기느냐는 것이다.
사람이 모든 농담을 보고 점수를 매길 수 있지만 수 천개의 응답을 생성하는 수 천개의 프롬프트를 수 천번에 업데이트에 모두 사람이 관여할 수는 없다.

**RLHF** 접근법을 사용한다.
사람을 약간 참여시켜서 **보상 모델**이라고 부르는 별개의 신경망을 훈련시키는 것이다.
사람에게 롤아웃을 점수 매기도록 요청하고 신경망을 사용해서 사람 점수를 모방한다.
그러고나면 실제 사람에게 묻는 대신 시뮬레이션된 사람(보상 모델)에 묻는다.
완벽한 인간은 아니지만, 통계적으로 인간 판단과 유사할 것이다.

보상 모델 훈련을 다음 그림을 통해 간단히 살펴보자.
![[Pasted image 20260206001433.png]]
프롬프트에 다섯 개의 롤아웃을 받고, 사람이 이 농담의 순위를 매긴다.
그리고 별개로, 보상 모델에 프롬프트와 농담을 전달하고 이 농담들에 점수를 매기도록 한다.
그리고 나서 보상 모델이 준 점수와 사람이 준 순서를 비교한다.

이 계산은 손실 함수를 설정하고 일치도를 계산하고 그것에 기반해 모델을 업데이트한다.
예시에서 사람이 2등을 준 농담의 점수가 너무 낮다. 그러면 해당 농담은 업데이트 이후 점수가 0.15 정도로 올라갈 수 있다. 신경망 훈련 과정을 사용해 모델의 예측을 조정하고, 인간 순서와 일치하게 한다. 그렇게 업데이트를 거치면 인간 선호도의 시뮬레이터로서 사용할 수 있게 된다.