텍스트를 tokenize한 뒤 토큰으로 이루어진 시퀀스를 신경망에 전달해 훈련시키는데,
이 단계에서의 목표는 이 **토큰들이 시퀀스에서 서로 어떻게 따라오는지의 통계적 관계를 모델링**하는 것이다.

3962 153 23
153 153 23
17788 153 23

입력된 시퀀스(윈도우 단위)에서 이런 패턴이 보인다면, 훈련 이후 이 윈도우에서 153으로 끝나는 시퀀스가 입력으로 주어졌을 때 신경망은 다음 토큰으로 23을 출력시킬 확률이 높아진다.


신경망 훈련을 위해서 
첫 번째로 토큰의 윈도우를 취한다.
데이터셋에서 상당히 무작위로 취하고, 윈도우 크기는 임의 설정이 가능하지만 긴 윈도우는
계산 비용이 비싸서 8000,4000,16000 등 적정 선에서 자른다

![[Pasted image 20260201014652.png]]

예를 들어 4개의 토큰 윈도우를 취했다고 하자.
그러면 이 4개의 토큰을 **context**라고 부른다.
이것이 신경망의 입력이 된다. 그리고 토큰의 가짓수가 100,277개가 있으므로
신경망은 그만큼의 숫자를 출력할 수 있고, 그 숫자는 해당 토큰이 시퀀스에서 다음으로 올 확률에 해당한다. 다음에 무슨 토큰이 올 지 추측하는 것이다.

처음에 신경망은 무작위로 초기화되어 있다.
이 예시에서 정답은 post인데 direction이 4%, post가 3%로 direction이 올 확률이 높게 설정되어 있다. 이 알고 있는 정답을 **label**이라고 한다.
기본적으로 이 post의 확률이 더 높아지고, 나머지의 확률은 낮아지게 하는 것이 목표.
수학적인 방법으로 다음에 오는 정확한 토큰에 더 높은 확률을 부여하도록 신경망을 업데이트한다. 이것이 훈련이다

이 과정은 이 윈도우의 이 토큰에서만 이루어지는 것이 아니라, 많은 윈도우를 샘플링하고
각각에서 이 과정이 이루어진다.

#### GPT
##### 전체 훈련 데이터는 이렇게 만들어짐

이론적으로는:
{([t1…tL],tL+1),([t2…tL+1],tL+2),… }\{([t_1 \dots t_L], t_{L+1}), ([t_2 \dots t_{L+1}], t_{L+2}), \dots\}{([t1​…tL​],tL+1​),([t2​…tL+1​],tL+2​),…}

실제로는:
- 전부 다 쓰지 않고
- **랜덤한 시작 위치 iii** 를 샘플링해서
- **미니배치**로 병렬 학습