강화학습이  등장한 것은 아주 최근의 일이 아니다. 이미 딥마인드의 알파고에서 사용했던 방법이다.
![[Pasted image 20260205235031.png]]
이 그림은 이세돌, RL을 사용한 모델, 지도학습 모델의 Elo rating을 비교한 것이다.
지도학습 모델은 인간 전문가 선수를 모방하는데, 그 방법으로는 인간 전문가 이상의 실력을 낼 수 없다. 하지만 강화학습 과정은 훨씬 더 강력한데,
바둑에서 강화학습은 시스템이 경험적으로, 통계적으로 게임에서 이기는 수를 둔다는 것을 의미한다.

알파고는 자기 자신과 대국하며 강화학습을 사용해서 롤아웃을 만드는 시스템이다.
고정된 바둑 게임을 진행하기에 프롬프트는 없지만, 많은 풀이를 시도하고,
특정 답 대신 승리로 이어지는 게임이 강화된다.

강화학습은 인간의 행동 분포에서 벗어나는 행동을 할 수 있게 하는데, 그 예가 알파고의 37수다.
인간 선수가 이 수를 놓을 확률은 1만 분의 1로, 인간이 둘 확률이 희박했다.

RL이 정확히 어떤 구조로 생겼는지는 알려져 있지 않다.
모델은 인간이 만들 수 없는 유추를 할 수도 있고, 새로운 사고 전략을 만들어낼 수도 있다.
아니면 완전히 새로운 언어를 만들어낼 수도 있다.  그러한 제약은 없다.
하지만 이 모든 것은 전략을 정제하고 완성할 수 있는 매우 크고 다양한 문제 집합이 있어야 가능하다. 크고 다양한 프롬프트 분포가 필요하다.

