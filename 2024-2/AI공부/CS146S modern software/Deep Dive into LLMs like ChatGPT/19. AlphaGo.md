강화학습이  등장한 것은 아주 최근의 일이 아니다. 이미 딥마인드의 알파고에서 사용했던 방법이다.
![[Pasted image 20260205235031.png]]
이 그림은 이세돌, RL을 사용한 모델, 지도학습 모델의 Elo rating을 비교한 것이다.
지도학습 모델은 인간 전문가 선수를 모방하는데, 그 방법으로는 인간 전문가 이상의 실력을 낼 수 없다. 하지만 강화학습 과정은 훨씬 더 강력한데,
바둑에서 강화학습은 시스템이 경험적으로, 통계적으로 게임에서 이기는 수를 둔다는 것을 의미한다.

알파고는 자기 자신과 대국하며 강화학습을 사용해서 롤아웃을 만드는 시스템이다.
고정된 바둑 게임을 진행하기에 프롬프트는 없지만, 많은 풀이를 시도하고,
특정 답 대신 승리로 이어지는 게임이 강화된다.