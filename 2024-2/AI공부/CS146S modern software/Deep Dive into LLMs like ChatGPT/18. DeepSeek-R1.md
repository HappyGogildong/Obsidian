딥시크 R1의 논문은 강화학습 미세조정에 대해 자세하게 다룬 논문 중 하나이다.
![[Pasted image 20260205223631.png]]
이것은 수학 문제를 푸는 것이 개선되어가는 그래프이다. 수천 번의 업데이트를 반복하며 정확도가 계속 올라간다.
![[Pasted image 20260205223828.png]]
여기 그래프를 보면, 최적화 후반에 모델의 **응답당 평균 길이가 올라가는 것**을 볼 수 있다.
모델이 더 높은 정확도를 얻기 위해 더 많은 토큰을 사용하는 것처럼 보인다.

![[Pasted image 20260205224002.png]]
저 빨간 글씨 부근에서 모델은 단계를 재평가하고 있다.
정확도를 위해 많은 아이디어를 시도하고, 다른 관점에서 시도하고, 되추적, 재구성, 백트래킹을 하는 것이 더 낫다는 것을 배웠다.
이는 사람이 수학 문제 해결 과정에 사용하는 것과 비슷하다.
모델은 **Chain of Thought**을 배우고 있다. 

여기서 놀라운 것은 모델이 생각하는 방법을 발견하고 있다는 것이다.
안드레이가 Cognitive Strategies라 부르는 것들, 문제를 어떻게 조작하고 다른 관점에서 접근하는지, 유추를 하고 문제를 푸는 방법 등을 발견한다. 
제공된 것은 정답뿐이고, 최적화하는 과정에서 이런 것들이 나온다.

![[Pasted image 20260205225720.png]]
이것은 RL을 적용하지 않은 ChatGPT 4o 모델의 응답
![[Pasted image 20260205225832.png]]
이것은 RL을 적용한 딥시크 R1 모델의 응답이다.
RL을 적용한 모델은 이런 사고 과정을 거치고 사람을 위한 깔끔한 풀이를 작성한다.

강화학습 과정에서 모델의 사고 과정을 얻는다.
토큰 시퀀스의 길이를 부풀리는 것이다. 생각하고 다른 방법을 시도하고는 것을 통해 문제 해결에 높은 정확도를 얻는다.

OpenAI의 o3 mini high는 추론 과정을 전부 보여주지 않고 요약해서 보여주는데, 
이는 distillation attack 때문이다. CoT를 흉내내는 것 마능로